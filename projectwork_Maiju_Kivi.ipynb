{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to categorize text\n",
    "\n",
    "Written text is one of humankind's greatest inventions. It enables us to interact with other people across time and space. You can write down your thoughts and a hundred years later another human can pick up your scribblings and read what you wrote. But how will that other human interpret the text? Will they consider it a news article describing some interesting event that occured? Will they consider it a fictional story, a personal letter, or even a divinely inspired scripture? \n",
    "\n",
    "This project aims to build a supervised feed-forward neural network that can do the interpretation for us. A basic feed-forward neural network takes in some input, processes it in the so-called hidden layer, and produces an output. In a supervised setting the correctness of the output will then be checked and if it is not the desired one, adjustments will be made to help the algorithm achieve better results.\n",
    "\n",
    "### Data\n",
    "As our data we'll use from the NLTK library the Brown corpus which contains documents from 15 different categories. The documents will be further chopped up into 100 word snippets. For the sake of uniformity this means that some data will be discarded if and when a document is not divisible by hundred. The snippets will then be randomly shuffled before dividing them into sets: 20% for the test set, and 80% for the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "documents = [(list(brown.words(fileid)), category) \n",
    "             for category in brown.categories() for fileid in brown.fileids(category)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_snippets(document_list):\n",
    "    snippets = []\n",
    "    for text_and_category in document_list:\n",
    "        text = text_and_category[0]\n",
    "        category = text_and_category[1]\n",
    "        lower_limit = 0\n",
    "        upper_limit = 100\n",
    "        while upper_limit < len(text):\n",
    "            snippet = text[lower_limit:upper_limit]\n",
    "            snippets.append((snippet, category))\n",
    "            lower_limit += 100\n",
    "            upper_limit += 100\n",
    "    return snippets\n",
    "\n",
    "snippets = get_snippets(documents)\n",
    "random.shuffle(snippets)\n",
    "size = int(len(snippets)*0.2)\n",
    "test_snippets = snippets[:size]\n",
    "train_snippets = snippets[size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "\n",
    "Given a piece of text, how would a human decide what type of text it is? While a poem might be easy to recognise (if it rhymes), how does one differentiate science fiction from romance fiction? The answer is in the words. We would see words like 'starship' or 'time machine' and based on previous knowledge of the likelihood of these words appearing in a science fiction novel versus the likelihood of the same words appearing in a romance novel, we would choose the more likely option. Although things aren't always this clear cut (what is there preventing a text being science fiction and romance?), it is a good starting point. Additionally, the most commonly used words such as 'the' and 'you' appear in virtually every text, so to lessen the workload, we can exclude those words from our considerations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = nltk.FreqDist(w.lower() for w in brown.words())\n",
    "most_common = [w for w, f in all_words.most_common(350)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There already exists a machine learnign algorithm that can classify text into different categories based on prior probabilities: the Naive Bayes Classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_naive_features(text_snippet):\n",
    "    featureset = {}\n",
    "    words = set(text_snippet[0])\n",
    "    for word in words:\n",
    "        if word.lower() not in most_common:\n",
    "            featureset['contains({})'.format(word.lower())] = (word in words)\n",
    "    return featureset\n",
    "\n",
    "featuresets = [(get_naive_features(d), c) for (d, c) in snippets]\n",
    "train_set, test_set = featuresets[size:], featuresets[:size]\n",
    "classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
    "print(\"Accuracy of the Naive Bayes Classifier: {:.2f}%\".format((nltk.classify.accuracy(\n",
    "    classifier, test_set)*100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the Naive Bayes Classifier that NLTK offers works very poorly for this task (only around 16% accuracy rate). This is because while the Naive Bayes Classifier does look at the words of the text snippet, the words themselves don't carry any weight: it is simply a yes or no, and this doesn't translate very well to deciding between 15 different categories. The solution? We will create our own supervised machine learning algorithm. Let's start by further dividing the training set into separate sets for each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_learn = []\n",
    "train_bel = []\n",
    "train_lore = []\n",
    "train_news = []\n",
    "train_hobby = []\n",
    "train_gov = []\n",
    "train_rom = []\n",
    "train_adv = []\n",
    "train_fic = []\n",
    "train_edit = []\n",
    "train_mys = []\n",
    "train_rev = []\n",
    "train_rel = []\n",
    "train_humor = []\n",
    "train_scifi = []\n",
    "\n",
    "for snippet in train_snippets:\n",
    "    if snippet[1] == 'learned':\n",
    "        train_learn.append(snippet)\n",
    "    elif snippet[1] == 'belles_lettres':\n",
    "        train_bel.append(snippet)\n",
    "    elif snippet[1] == 'lore':\n",
    "        train_lore.append(snippet)\n",
    "    elif snippet[1] == 'news':\n",
    "        train_news.append(snippet)\n",
    "    elif snippet[1] == 'hobbies':\n",
    "        train_hobby.append(snippet)\n",
    "    elif snippet[1] == 'government':\n",
    "        train_gov.append(snippet)\n",
    "    elif snippet[1] == 'romance':\n",
    "        train_rom.append(snippet)\n",
    "    elif snippet[1] == 'adventure':\n",
    "        train_adv.append(snippet)    \n",
    "    elif snippet[1] == 'fiction':\n",
    "        train_fic.append(snippet)\n",
    "    elif snippet[1] == 'editorial':\n",
    "        train_edit.append(snippet)\n",
    "    elif snippet[1] == 'mystery':\n",
    "        train_mys.append(snippet)\n",
    "    elif snippet[1] == 'reviews':\n",
    "        train_rev.append(snippet)\n",
    "    elif snippet[1] == 'religion':\n",
    "        train_rel.append(snippet)\n",
    "    elif snippet[1] == 'humor':\n",
    "        train_humor.append(snippet)\n",
    "    elif snippet[1] == 'science_fiction':\n",
    "        train_scifi.append(snippet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As established earlier, the key to categorizing text based on the words that appear in it lies with the prior probabilities. So how do we get them? A simple way to go about it is to count all the words that appear in a certain category (excluding the most common ones) and keep the most frequent words and their frequency as the relevant features. The units in the hidden layer of our neural network will use these frequencies in their computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(text_snippet, feature_set):\n",
    "    words = set(text_snippet[0])\n",
    "    for word in words:\n",
    "        if word.lower() not in most_common:\n",
    "            if word.lower() in feature_set:\n",
    "                feature_set[word.lower()] += 1 \n",
    "            else:\n",
    "                feature_set[word.lower()] = 1 \n",
    "\n",
    "def get_value(pair):\n",
    "    return pair[1]                \n",
    "\n",
    "\n",
    "def get_feature_set(train_subset):\n",
    "    features = {}\n",
    "    for snippet in train_subset:\n",
    "        get_features(snippet, features)\n",
    "    feature_set = {}\n",
    "    for key, value in sorted(features.items(), key=get_value, reverse=True)[:400]:\n",
    "        feature_set[key] = value\n",
    "    return feature_set\n",
    "\n",
    "learn_features = get_feature_set(train_learn)       \n",
    "bel_features = get_feature_set(train_bel)\n",
    "lore_features = get_feature_set(train_lore)\n",
    "news_features = get_feature_set(train_news)\n",
    "hobby_features = get_feature_set(train_hobby)\n",
    "gov_features = get_feature_set(train_gov)\n",
    "rom_features = get_feature_set(train_rom)\n",
    "adv_features = get_feature_set(train_adv)\n",
    "fic_features = get_feature_set(train_fic)\n",
    "edit_features = get_feature_set(train_edit)\n",
    "mys_features = get_feature_set(train_mys)\n",
    "rev_features = get_feature_set(train_rev)\n",
    "rel_features = get_feature_set(train_rel)\n",
    "humor_features = get_feature_set(train_humor)\n",
    "scifi_features = get_feature_set(train_scifi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of snippets in a given category varies greatly. Since the word frequencies will be used to make a decision about the category, this puts the least frequent categories at a disadvantage because their word frequencies will always be much smaller. To correct this, we'll normalize the frequencies so that each category 'has appeared' in the training data roughly the same amount of times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frequency = max(len(train_learn), len(train_bel), len(train_lore), len(train_news), len(train_hobby), \\\n",
    "                    len(train_gov), len(train_rom), len(train_adv), len(train_fic), len(train_edit), \\\n",
    "                    len(train_mys), len(train_rev), len(train_rel), len(train_humor), len(train_scifi))\n",
    "\n",
    "def normalize(train_subset, features):\n",
    "    factor = max_frequency/len(train_subset)\n",
    "    if factor > 1.5:\n",
    "        factor = math.ceil(factor)\n",
    "    else:\n",
    "        factor = math.floor(factor)\n",
    "    for key, value in features.items():\n",
    "        new_value = value*factor\n",
    "        features[key] = new_value\n",
    "    return features\n",
    "        \n",
    "\n",
    "learn_features = normalize(train_learn, learn_features)\n",
    "bel_features = normalize(train_bel, bel_features)\n",
    "lore_features = normalize(train_lore, lore_features)\n",
    "news_features = normalize(train_news, news_features)\n",
    "hobby_features = normalize(train_hobby, hobby_features)\n",
    "gov_features = normalize(train_gov, gov_features)\n",
    "rom_features = normalize(train_rom, rom_features)\n",
    "adv_features = normalize(train_adv, adv_features)\n",
    "fic_features = normalize(train_fic, fic_features)\n",
    "edit_features = normalize(train_edit, edit_features)\n",
    "mys_features = normalize(train_mys, mys_features)\n",
    "rev_features = normalize(train_rev, rev_features)\n",
    "rel_features = normalize(train_rel, rel_features)\n",
    "humor_features = normalize(train_humor, humor_features)\n",
    "scifi_features = normalize(train_scifi, scifi_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have the prior probabilities so it is time to set up the hidden layer of our neural network. There will be 15 units in the hidden layer, each of them counting a comparison number for a category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(snippet, features):\n",
    "    total = 0\n",
    "    for word in snippet:\n",
    "        if word.lower() in features:\n",
    "            total += features[word.lower()]\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output unit of our neural network takes in all the comparison numbers from the hidden layer and simply picks the one that is highest. It then outputs the name of the corresponding category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(words):\n",
    "    options = {}\n",
    "    options['learned'] = count(words, learn_features)\n",
    "    options['belles_lettres'] = count(words, bel_features)\n",
    "    options['lore'] = count(words, lore_features)\n",
    "    options['news'] = count(words, news_features)\n",
    "    options['hobbies'] = count(words, hobby_features)\n",
    "    options['government'] = count(words, gov_features)\n",
    "    options['romance'] = count(words, rom_features)\n",
    "    options['adventure'] = count(words, adv_features)\n",
    "    options['fiction'] = count(words, fic_features)\n",
    "    options['editorial'] = count(words, edit_features)\n",
    "    options['mystery'] = count(words, mys_features)\n",
    "    options['reviews'] = count(words, rev_features)\n",
    "    options['religion'] = count(words, rel_features)\n",
    "    options['humor'] = count(words, humor_features)\n",
    "    options['science_fiction'] = count(words, scifi_features)\n",
    "    high_score = 0\n",
    "    prediction = ''\n",
    "    for key, value in options.items():\n",
    "        if value > high_score:\n",
    "            high_score = value\n",
    "            prediction = key\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a feed-forward network, no information will be passed back to the units, and therefore it cannot improve its performance on its own. However, since we already are supervising the learning process, we can help out by adjusting the features, i.e. word frequencies, whenever our neural network makes a mistake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_weight(snippet, features):\n",
    "    for word in snippet:\n",
    "        if word.lower() in features:\n",
    "            features[word.lower()] += 1\n",
    "            \n",
    "            \n",
    "def adjust_features(snippet):\n",
    "    words = snippet[0]\n",
    "    category = snippet[1]\n",
    "    prediction = get_prediction(words)\n",
    "    if prediction != category:\n",
    "        if category == 'learned':\n",
    "            add_weight(words, learn_features)\n",
    "        elif category == 'belles_lettres':\n",
    "            add_weight(words, bel_features)\n",
    "        elif category == 'lore':\n",
    "            add_weight(words, lore_features)\n",
    "        elif category == 'news':\n",
    "            add_weight(words, news_features)\n",
    "        elif category == 'hobbies':\n",
    "            add_weight(words, hobby_features)\n",
    "        elif category == 'government':\n",
    "            add_weight(words, gov_features)\n",
    "        elif category == 'romance':\n",
    "            add_weight(words, rom_features)\n",
    "        elif category == 'adventure':\n",
    "            add_weight(words, adv_features)\n",
    "        elif category == 'fiction':\n",
    "            add_weight(words, fic_features)\n",
    "        elif category == 'editorial':\n",
    "            add_weight(words, edit_features)\n",
    "        elif category == 'mystery':\n",
    "            add_weight(words, mys_features)\n",
    "        elif category == 'reviews':\n",
    "            add_weight(words, rev_features)\n",
    "        elif category == 'religion':\n",
    "            add_weight(words, rel_features)\n",
    "        elif category == 'humor':\n",
    "            add_weight(words, humor_features)\n",
    "        elif category == 'science_fiction':\n",
    "            add_weight(words, scifi_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have set up everything so it's time for the network to start the learning process. The network will go through all the text snippets in the training data set and we'll adjust the features whenever a mistake is made. This process, called an epoch, will be repeated twenty times. As this takes some time, the network will keep us informed of it's progress every five epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Training started.')\n",
    "epoch = 0\n",
    "while epoch < 20:\n",
    "    for snippet in train_snippets:\n",
    "        adjust_features(snippet)\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print('Finished epoch {}/20.'.format(epoch+1))\n",
    "    random.shuffle(train_snippets)\n",
    "    epoch += 1\n",
    "print('Training finished.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation\n",
    "\n",
    "Our neural network has finished its training and it's time to see how well it performs on the test data that it has never seen before. The test results will be presented as a confusion matrix: the rows represent the correct category and the columns represent the category that our network picked. The diagonal line from the upper left corner to the bottom right corner contains all the instances where our network got the category right. Due to spacing issues the confusion matrix will not show percentages but the number of snippets that it classified as X. However, we will display the overall accuracy under the confusion matrix along with the accuracy achieved with the NLTK Naive Bayes Classifier, the accuracy with the naive baseline (that is, caegorizing everything as the most frequent category), and the accuracy achieved by simply guessing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for snippet in test_snippets:\n",
    "    text = snippet[0]\n",
    "    correct_category = snippet[1]\n",
    "    predicted_category = get_prediction(text)\n",
    "    test.append(predicted_category)\n",
    "gold = [category for (text, category) in test_snippets]  \n",
    "cm = nltk.ConfusionMatrix(gold, test)\n",
    "print(cm.pretty_format(sort_by_count=True))\n",
    "\n",
    "print('The number of snippets in the test set: {}'.format(len(test_snippets)))\n",
    "categories_by_frequency = nltk.FreqDist(category for (text, category) in snippets)\n",
    "print(categories_by_frequency.most_common(), '\\n')\n",
    "accuracy = 0\n",
    "for i in range(len(gold)):\n",
    "    if gold[i] == test[i]:\n",
    "        accuracy += 1       \n",
    "print('Accuracy of our neural network: {:.2f}%'.format((accuracy/len(gold))*100))\n",
    "\n",
    "naive = categories_by_frequency.most_common()[0][0]\n",
    "naive_test = []\n",
    "while len(naive_test) != len(gold):\n",
    "    naive_test.append(naive)\n",
    "naive_accuracy = 0\n",
    "for i in range(len(gold)):\n",
    "    if gold[i] == naive_test[i]:\n",
    "        naive_accuracy += 1       \n",
    "print('Accuracy achieved with naive baseline approach: {:.2f}%'.format((naive_accuracy/len(gold))*100))\n",
    "print(\"Accuracy of the Naive Bayes Classifier: {:.2f}%\".format((nltk.classify.accuracy(\n",
    "    classifier, test_set)*100)))\n",
    "print('Accuracy achieved by guessing: {:.2f}%'.format((1/15)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, our neural network works much, much better (around 50% accuracy) than any of the other three methods (around 15% accuracy for the naive baseline, around 16% accuracy for the Naive Bayes Classifier, and 6.67% accuracy by guessing). Unsurprisingly, the machine has a hard time understanding 'humor'. Ironically, also 'science_fiction' appears to be difficult to get right. The best recognized categories seem to be 'learned' and 'belles_lettres', although 'belles_lettres' also seems to cause much confusion: the algorithm sorts numerous snippets that belong to other categories into 'belles_lettres'. This boils down to the fact that even after excluding the most common words, many words still appear in all categories and 'belles_lettres' seems to have the highest frequencies for those words so that's what the neural network picks as the most likely category.\n",
    "\n",
    "The categories that are the hardest and easiest to get right are respectively the two least and most frequent categories. It appears that despite our best efforts to normalize the word frequencies, it isn't enough. We would need more actual data to get better results. \n",
    "\n",
    "### Conclusion \n",
    "\n",
    "What can we learn from this project? Our neural network performs significantly better than the Naive Bayes Classifier although they aren't that different. They both look at the words in a text snippet and check if those words have appeared in a certain category before. The Naive Bayes Classifier gets back a simple yes or no answer, whereas our neural network will get back a more detailed answer: 'yes, 248 times', 'no, never', or 'yes, but only 3 times'. This comes back to the amount of data available: the more times you've come across some event in a particular setting, the more certain you can be that it is connected to that setting. However, with more data there will also be more noise that needs to be weeded out so it is not as simple as 'the more data, the better'. Yes, a large volume of data is a good start, but the more important factor is picking the right features from that data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
